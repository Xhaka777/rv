import { Alert, Image, StyleSheet, View, AppState, Linking, TouchableOpacity, Text } from 'react-native';
import React, { useCallback, useEffect, useRef, useState } from 'react';
import { createBottomTabNavigator } from '@react-navigation/bottom-tabs';
import { Settings, TrustedContacts, LiveStream, SafeZone, Premium, HeadsUp, SafeWord, SafeWordTraining, PasscodeSettings, HowToUse, Footages, BodyCam, HeadsUpSettings, SiriSetupScreen } from '../screens';
import { Images, Metrix, NavigationService, Utills } from '../config';
import { MD3LightTheme, PaperProvider } from 'react-native-paper';
import notifee from '@notifee/react-native';
import { CustomModal, CustomText, PrimaryButton } from '../components';
import { useDispatch, useSelector } from 'react-redux';
import { RootState } from '../redux/reducers';
import { HomeActions } from '../redux/actions';
import { Environments } from '../services/config';
import VoiceDetectionUI from '../components/VoiceDetectionUI';
import { useFocusEffect, useNavigationState } from '@react-navigation/native';
import BackgroundService from 'react-native-background-actions';
import FakeLockScreen from '../components/FakeLockScreen';
import { isThreatWithinRadius } from '../utils/distanceHelpers';
import { HomeAPIS } from '../services/home';
import { store } from '../redux/Store';
import { NativeModules, NativeEventEmitter } from 'react-native';

//Import the new AudioSessionService with interruption handling
import {
  audioSessionService,
  AudioStreamDataEvent,
  SpeechRecognizedEvent,
  MicrophoneInterruptionEvent,
  MicrophoneStatusChangedEvent,
  MicrophoneDebugLogEvent,
  AudioInterruptionEvent,
  SiriInterruptionEvent,
  HeySiriDetectedEvent
} from '../utils/AudioSessionService';

const { SiriEventEmitter } = NativeModules;

const Tab = createBottomTabNavigator();

type TabStackType = {
  name: string;
  component: React.FC;
  active: any;
  inActive: any;
  mode?: 'AUDIO' | 'VIDEO';
}[];

const tabsData: TabStackType = [
  {
    name: 'HeadsUp',
    component: HeadsUp,
    active: Images.HeadsUp,
    inActive: Images.HeadsUp,
    mode: 'AUDIO',
  },
  {
    name: 'LiveStream',
    component: LiveStream,
    active: Images.HomeActive,
    inActive: Images.HomeActive,
    mode: 'VIDEO',
  },
  {
    name: 'Premium',
    component: Premium,
    active: Images.Premium,
    inActive: Images.Premium,
    mode: 'AUDIO',
  },
  {
    name: 'Settings',
    component: Settings,
    active: Images.SettingsActive,
    inActive: Images.SettingsActive,
    mode: 'AUDIO',
  },
];

export const TabStack: React.FC = () => {

  const dispatch = useDispatch();
  const selectedModel = useSelector((state: RootState) => state.home.selectedModel);
  const userDetails = useSelector((state: RootState) => state.home.userDetails);
  const sw = useSelector((state: RootState) => state.home.safeWord?.isSafeWord);
  const sz = useSelector((state: RootState) => state.home.isSafeZone);
  const safeWord = useSelector((state: RootState) => state.home.safeWord?.safeWord);
  const isAudioStreamStopped = useSelector((state: RootState) => state.home.streamStopped);
  const showFakeLockScreen = useSelector((state: RootState) => state.home.showFakeLockScreen);

  //  EXACT SAME STATE AS OLD APPROACH
  const averageHeartRate = 75; // Example average heart rate
  const [isVisible, setIsVisible] = useState(false);
  const [isSafeWord, setIsSafeWord] = useState(sw);
  const [isSafeZone, setIsSafeZone] = useState(sz);
  const [currentModel, setCurrentModel] = useState(selectedModel);

  const gender = userDetails?.user?.gender && userDetails.user.gender.length > 0
    ? userDetails.user.gender
    : 'male';

  //  EXACT SAME AUDIO NAME STRING AS OLD APPROACH
  const audioNameString =
    userDetails?.user?.first_name +
    userDetails?.user?.last_name +
    '-' +
    gender +
    '-' +
    safeWord;

  // EXACT SAME WEBSOCKET REF AS OLD APPROACH
  const ws = useRef<WebSocket | null>(null);

  // UI states
  const navigationState = useNavigationState(state => state);
  const tabState = navigationState?.routes?.[navigationState.index]?.state;

  const currentTabName = tabState?.routes?.[tabState?.index]?.name || 'LiveStream';
  console.log('Full navigation state:', navigationState);
  console.log('Navigation index:', navigationState?.index);
  console.log('Current route:', navigationState?.routes?.[navigationState.index]);

  const threatAlertMode = useSelector((state: RootState) => state.home.threatAlertMode);
  const headsUpRadius = useSelector((state: RootState) => state.home.headsUpRadius || 3);
  const userLocation = useSelector((state: RootState) => state.home.userLocation);
  const [hasThreatsInRadius, setHasThreatsInRadius] = useState(false);

  const cameraMode = useSelector((state: RootState) => state.home.cameraMode || 'AUDIO');
  const isVideoMode = cameraMode === 'VIDEO';
  const isOnLiveStreamTab = currentTabName === 'LiveStream';

  // Voice UI states
  const [showVoiceUI, setShowVoiceUI] = useState(false);
  const [voiceUIState, setVoiceUIState] = useState<'listening' | 'processing' | 'detected'>('listening');
  const [audioLevel, setAudioLevel] = useState(0.5);

  // EXACT SAME WEBSOCKET STATES AS OLD APPROACH
  const [isWebSocketActive, setIsWebSocketActive] = useState(false);
  const wsRef = useRef<WebSocket | null>(null);
  const audioStreamRef = useRef<boolean>(false);

  // NEW: Enhanced interruption states
  const [isAudioServicesSuspended, setIsAudioServicesSuspended] = useState(false);
  const [lastInterruptionType, setLastInterruptionType] = useState<string>('');
  const [debugLogs, setDebugLogs] = useState<string[]>([]);

  // Armed timer
  const [armingTimerId, setArmingTimerId] = useState<NodeJS.Timeout | null>(null);
  const activeTimer = useSelector((state: RootState) => state.home.activeTimer || '30m');

  //Siri Detection...
  const [showSiriBlockedModal, setShowSiriBlockedModal] = useState(false);


  console.log('inside the tabstack with enhanced interruption handling')

  const timerToMilliseconds = (timer: string): number => {
    const minutes = parseInt(timer.replace('m', ''));
    return minutes * 60 * 1000;
  };

  const startArmingTimer = useCallback(() => {
    if (armingTimerId) {
      clearTimeout(armingTimerId);
    }

    const duration = timerToMilliseconds(activeTimer);
    const timerId = setTimeout(() => {
      console.log('ðŸ•’ Arming timer expired - automatically disarming');
      dispatch(HomeActions.setShowFakeLockScreen(false));
      onDisplayNotification(
        'Bodycam Timer Expired',
        `Your ${activeTimer} protection period has ended`
      );
      setArmingTimerId(null);
    }, duration);
    setArmingTimerId(timerId);
  }, [activeTimer, armingTimerId, dispatch]);

  useEffect(() => {
    const requestNotificationPermission = async () => {
      try {
        console.log('ðŸ”” Requesting notification permission on TabStack mount...');
        await notifee.requestPermission();
        console.log('âœ… Notification permission requested');
      } catch (error) {
        console.error('âŒ Failed to request notification permission:', error);
      }
    };

    // Request notification permission immediately when TabStack mounts
    requestNotificationPermission();
  }, []);

  useEffect(() => {
    const handleAppStateChange = async (nextAppState: string) => {
      console.log('ðŸ“± App state changed to:', nextAppState);

      // NEW: Dismiss FakeLockScreen when app goes to background
      if (nextAppState === 'background' || nextAppState === 'inactive') {
        console.log('ðŸ“± App went to background/inactive - dismissing FakeLockScreen');
        if (showFakeLockScreen) {
          dispatch(HomeActions.setShowFakeLockScreen(false));
        }
      }

      if (nextAppState === 'active') {
        // App became active - check if we should have mic services running
        if (isSafeWord && !isSafeZone && !isAudioServicesSuspended) {
          console.log('ðŸ”„ App active - checking microphone recovery...');

          // Small delay to let system settle
          setTimeout(async () => {
            try {
              const result = await audioSessionService.forceRecoverMicrophone();
              console.log('ðŸ”„ Recovery result:', result);

              if (!result.success) {
                console.log('âŒ Recovery failed - restarting services manually');
                // Fallback: restart services from scratch
                await stopAllAudioServices();
                setTimeout(() => {
                  startStreaming();
                }, 500);
              }
            } catch (error) {
              console.error('âŒ Recovery attempt failed:', error);
            }
          }, 1000);
        }
      }
    };

    const subscription = AppState.addEventListener('change', handleAppStateChange);
    return () => subscription?.remove();
  }, [isSafeWord, isSafeZone, isAudioServicesSuspended, showFakeLockScreen, dispatch]);

  useEffect(() => {
    console.log('ðŸ—£ï¸ Setting up Hey Siri detection listener...');

    const heySiriListener = audioSessionService.addListener('HeySiriDetected', (event: HeySiriDetectedEvent) => {
      console.log('ðŸš¨ HEY SIRI DETECTED WHILE APP HAS MIC!', event);

      // Check if app is in background/inactive
      if (AppState.currentState === 'background' || AppState.currentState === 'inactive') {
        // Show notification instead of modal
        onDisplayNotification(
          'Siri access blocked',
          'Siri voice activation wonâ€™t work while Rove is active. Hold the side button to activate Siri'
        );
      } else {
        // App is active - show modal as before
        console.log('ðŸ“± App currently has microphone, blocking Siri activation');
        setShowSiriBlockedModal(true);
      }
    });

    return () => {
      console.log('ðŸ§¹ Cleaning up Hey Siri detection listener');
      heySiriListener.remove();
    };
  }, []);

  useEffect(() => {
    if (SiriEventEmitter) {
      const siriEmitter = new NativeEventEmitter(SiriEventEmitter);
      const sub = siriEmitter.addListener('RoveArmCommand', (data) => {
        console.log('ðŸš€ Siri command received in JS:', data);
        handleArmCommand(data.source || 'siri');
      });
      return () => sub.remove();
    }
  }, []);

  useEffect(() => {
    if (showFakeLockScreen) {
      startArmingTimer();
    } else {
      if (armingTimerId) {
        console.log('ðŸ›‘ Clearing arming timer (manual disarm)');
        clearTimeout(armingTimerId);
        setArmingTimerId(null);
      }
    }
  }, [showFakeLockScreen]);

  useEffect(() => {
    return () => {
      if (armingTimerId) {
        clearTimeout(armingTimerId);
      }
    }
  }, [armingTimerId]);

  // Deep linking
  useEffect(() => {
    console.log('ðŸ”— Setting up deep link listener...');

    const linkingSubscription = Linking.addEventListener('url', (event) => {
      console.log('ðŸ”— Deep link received:', event.url);
      if (event.url === 'rove://arm') {
        handleArmCommand('deeplink');
      }
    });

    Linking.getInitialURL().then(url => {
      if (url === 'rove://arm') {
        setTimeout(() => handleArmCommand('deeplink-initial'), 1000);
      }
    });

    return () => {
      linkingSubscription?.remove();
    };
  }, [dispatch]);

  const handleArmCommand = async (source: string) => {
    console.log(`ðŸš¨ ARM COMMAND from ${source}!`);
    // Alert.alert('ARM COMMAND', `Received from ${source}`);

    // Set flag to arm when LiveStream loads
    dispatch(HomeActions.setShouldArmOnLivestream(true));

    // Navigate to LiveStream
    NavigationService.navigate('LiveStream');
  };

  // ENHANCED: Cleanup effect with enhanced audio session service
  useEffect(() => {
    if (isAudioStreamStopped) {
      console.log('ðŸ”´ Audio stream stopped - cleaning up enhanced audio services');
      stopAllAudioServices();
    }

    return () => {
      console.log('ðŸ§¹ TabStack cleanup function called');
      if (ws.current) {
        console.log('ðŸ”´ Closing WebSocket connection');
        ws.current.close();
        ws.current = null;
      }
      console.log('ðŸ”´ Final cleanup - stopping all enhanced audio services');
      stopAllAudioServices();
    };
  }, []);

  // Helper function to stop all audio services
  const stopAllAudioServices = useCallback(async () => {
    try {
      await audioSessionService.stopAudioStreaming();
      await audioSessionService.stopRecording();
      await audioSessionService.stopSpeechRecognition();
      console.log('âœ… All enhanced audio services stopped');
    } catch (error) {
      console.error('âŒ Error stopping audio services:', error);
    }
  }, []);

  // EXACT SAME MODEL UPDATE AS OLD APPROACH
  useEffect(() => {
    setCurrentModel(selectedModel);
  }, [selectedModel]);

  // EXACT SAME THREAT DETECTION FUNCTIONS AS OLD APPROACH
  const threatDetected = () => {
    dispatch(HomeActions.setThreatDetected(true));
    onDisplayNotification(
      'A New Threat Detected',
      'Start your live stream now',
    );
  };

  const appTriggeredDetected = () => {
    dispatch(HomeActions.setAppTriggered(true));
    onDisplayNotification(
      'App Triggered Detection',
      'Starting your live stream now',
    );
  };

  //  EXACT SAME STATE EFFECTS AS OLD APPROACH
  useEffect(() => {
    console.log('ðŸ”„ isSafeWord changed from', isSafeWord, 'to', sw);
    setIsSafeWord(sw);
  }, [sw]);

  useEffect(() => {
    setIsSafeZone(sz);
  }, [sz]);

  // EXACT SAME NOTIFICATION FUNCTION AS OLD APPROACH
  const onDisplayNotification = async (title: string, body: string) => {

    const channelId = await notifee.createChannel({
      id: 'default',
      name: 'Default Channel',
    });

    await notifee.displayNotification({
      title: title,
      body: body,
      android: {
        channelId,
        pressAction: {
          id: 'default',
        },
      },
    });
  };

  console.log('safeWord', safeWord);

  // EXACT SAME WEBSOCKET SETUP AS OLD APPROACH
  const setupWebSocket = useCallback(() => {
    console.log('ðŸŒ Creating WebSocket connection to:', currentModel);
    console.log('ðŸ”— Will send audioNameString:', audioNameString);

    const socket = new WebSocket(currentModel);

    socket.onopen = () => {
      console.log('âœ… WebSocket connection opened successfully');
      console.log('ðŸ“¤ Sending audioNameString to server:', audioNameString);
      ws.current = socket;
      ws.current.send(audioNameString);
      console.log('âœ… AudioNameString sent to server');
    };

    socket.onmessage = event => {
      console.log('ðŸ“¨ Message received from server:', event.data);
      try {
        const parsedData = JSON.parse(event.data);
        console.log('ðŸ“Š Parsed server data:', parsedData);

        // ðŸ”¥ EXACT SAME APP TRIGGERED DETECTION AS OLD APPROACH
        if (parsedData.severity === 'APP triggered') {
          console.log('ðŸš¨ APP triggered detected - starting stream');
          appTriggeredDetected();
          return; // Exit early
        }

        // ðŸ”¥ EXACT SAME THREAT DETECTION LOGIC AS OLD APPROACH
        if (currentModel == Environments.Models.WHISPER_AND_SENTIMENT) {
          console.log('ðŸ§  Using WHISPER_AND_SENTIMENT model');
          const isNegativeSentiment = parsedData?.sentiment == 'negative';
          const threatScore = parsedData?.threat_level > 0.7 ? true : false;
          console.log('ðŸ“Š Sentiment analysis:', { isNegativeSentiment, threatScore });

          if (isNegativeSentiment && threatScore) {
            console.log('ðŸš¨ Threat detected via sentiment analysis');
            threatDetected();
          }
        } else {
          console.log('ðŸ§  Using standard threat detection model');
          const isThreat = parsedData['threat detected'];
          console.log('ðŸ” Threat detection result:', isThreat);

          if (isThreat) {
            console.log(`ðŸš¨ ${isThreat} is a threatening word.`);
            threatDetected();
          } else {
            console.log('âœ… No threat detected in this message');
          }
        }
      } catch (error) {
        console.error('âŒ Error parsing server message:', error);
      }
    };

    socket.onerror = error => {
      console.error('âŒ WebSocket error occurred:', error);
    };

    socket.onclose = (event) => {
      console.log('ðŸ”´ WebSocket connection closed. Code:', event.code, 'Reason:', event.reason);
    };
  }, [currentModel, audioNameString, isSafeZone]);

  // ðŸ”¥ EXACT SAME BACKGROUND TASK SETUP AS OLD APPROACH
  const options = {
    taskName: 'Background Audio',
    taskTitle: 'Background audio is running',
    taskDesc: 'Background audio is running to detect threats',
    taskIcon: {
      name: 'ic_launcher',
      type: 'mipmap',
    }
  };

  const backgroundTask = async (taskDataArguments: any) => {
    const { delay } = taskDataArguments;

    await new Promise(async (resolve) => {
      for (let i = 0; BackgroundService.isRunning(); i++) {
        console.log('Background task running', i);

        if (ws.current && ws.current.readyState === WebSocket.OPEN) {
          console.log('WebSocket is connected in background');
        }

        await BackgroundService.updateNotification({
          taskDesc: `Background task running: ${i}`,
        });

        await new Promise(resolve => setTimeout(resolve, delay || 1000));
      }
      resolve(undefined);
    });
  };

  // ðŸ”¥ NEW: Enhanced interruption handling functions
  const handleMicrophoneInterruption = useCallback((event: MicrophoneInterruptionEvent) => {
    console.log('ðŸš¨ MICROPHONE INTERRUPTION:', audioSessionService.getInterruptionTypeDescription(event.type));
    console.log('ðŸ“‹ Interruption details:', event);

    setLastInterruptionType(event.type);

    switch (event.type) {
      case 'began':
        setIsAudioServicesSuspended(true);
        onDisplayNotification(
          'Microphone Interrupted',
          'Another app is using the microphone'
        );
        break;

      case 'siri_began':
        setIsAudioServicesSuspended(true);
        console.log('ðŸ—£ï¸ Siri detected - services will resume automatically after Siri finishes');
        break;

      case 'ended':
      case 'siri_ended':
        setIsAudioServicesSuspended(false);
        console.log('âœ… Services resumed after interruption');
        break;

      case 'failed_resume':
        console.log('âŒ Failed to resume - may need manual restart');
        Alert.alert(
          'Audio Service Issue',
          'Failed to resume audio services. Tap to restart.',
          [
            { text: 'Restart', onPress: () => restartAudioServices() },
            { text: 'Cancel', style: 'cancel' }
          ]
        );
        break;
    }
  }, []);

  const handleDebugLog = useCallback((event: MicrophoneDebugLogEvent) => {
    const formattedLog = audioSessionService.formatDebugLog(event);
    console.log('ðŸ› DEBUG:', formattedLog);

    // Keep last 50 debug logs
    setDebugLogs(prev => [formattedLog, ...prev.slice(0, 49)]);

    // Check for specific patterns
    if (audioSessionService.detectSiriInterruption(event.message)) {
      console.log('ðŸ” Siri pattern detected in debug log');
    }

    if (audioSessionService.detectCallInterruption(event.message)) {
      console.log('ðŸ“ž Call pattern detected in debug log');
    }
  }, []);

  const restartAudioServices = useCallback(async () => {
    console.log('ðŸ”„ Manually restarting audio services...');

    try {
      // Stop all services first
      await stopAllAudioServices();

      // Wait a moment
      await new Promise(resolve => setTimeout(resolve, 500));

      // Restart streaming if it should be active
      if (isSafeWord && !isSafeZone) {
        await audioSessionService.startAudioStreaming();
        await audioSessionService.startSpeechRecognition();
        console.log('âœ… Audio services restarted successfully');
      }
    } catch (error) {
      console.error('âŒ Failed to restart audio services:', error);
      Alert.alert('Error', 'Failed to restart audio services');
    }
  }, [isSafeWord, isSafeZone, stopAllAudioServices]);

  // ðŸ”¥ ENHANCED: Streaming function with enhanced audio session service
  const startStreaming = useCallback(async () => {
    console.log('ðŸŽ¯ setupWebSocket called with enhanced interruption handling');
    setupWebSocket();

    // ðŸ”¥ ENHANCED: Setup enhanced audio session service with interruption handling
    console.log('ðŸŽ§ Setting up enhanced audio listeners...');

    // Setup enhanced audio streaming listeners
    audioSessionService.addListener('AudioStreamData', (data: AudioStreamDataEvent) => {
      console.log('Live audio is streaming from enhanced AudioSessionService========>>> 1');
      if (ws.current && ws.current.readyState === WebSocket.OPEN) {
        // Convert base64 to raw binary data like old approach
        try {
          const binaryData = atob(data.audioBuffer);
          ws.current.send(binaryData);
        } catch (error) {
          console.error('Error sending enhanced audio data:', error);
        }
      }
    });

    // ðŸ”¥ ENHANCED: Speech recognition listener for safe word detection with interruption awareness
    audioSessionService.addListener('SpeechRecognized', (data: SpeechRecognizedEvent) => {
      console.log('ðŸ—£ï¸ ENHANCED SPEECH DETECTED:', data.text);
      console.log('ðŸŽ¯ Current safe word:', safeWord);
      console.log('ðŸ” Is speech final?', data.isFinal);
      console.log('ðŸ” Are services suspended?', isAudioServicesSuspended);

      // Skip processing if services are suspended
      if (isAudioServicesSuspended) {
        console.log('âš ï¸ Services suspended, skipping safe word detection');
        return;
      }

      if (data.text && safeWord) {
        const spokenText = data.text.toLowerCase();
        const targetSafeWord = safeWord.toLowerCase();

        console.log('ðŸ” Comparing:', {
          spoken: spokenText,
          target: targetSafeWord,
          includes: spokenText.includes(targetSafeWord)
        });

        // Check if the safe word is contained in the spoken text
        if (spokenText.includes(targetSafeWord)) {
          console.log('ðŸš¨ SAFE WORD DETECTED! Starting stream...');
          showVoiceDetectionUI(4000);
          NavigationService.navigate('LiveStream', {
            autoStartStream: true,
            triggerReason: 'safe_word',
            detectedWord: safeWord
          });
        } else {
          console.log('âšª Safe word not detected in speech');
        }
      } else {
        console.log('âš ï¸ Missing data:', { hasText: !!data.text, hasSafeWord: !!safeWord });
      }
    });

    // ðŸ”¥ NEW: Enhanced interruption event listeners
    audioSessionService.addListener('microphoneInterruption', handleMicrophoneInterruption);

    audioSessionService.addListener('microphoneStatusChanged', (event: MicrophoneStatusChangedEvent) => {
      console.log('ðŸ“Š MICROPHONE STATUS CHANGED:', {
        isActive: event.isActive,
        reason: event.reason,
        timestamp: event.timestamp
      });
    });

    audioSessionService.addListener('microphoneDebugLog', handleDebugLog);

    // ðŸ”¥ NEW: Legacy interruption event listeners for compatibility
    audioSessionService.addListener('SiriInterruptionBegan', (event: SiriInterruptionEvent) => {
      console.log('ðŸ—£ï¸ LEGACY: Siri interruption began');
      setIsAudioServicesSuspended(true);
    });

    audioSessionService.addListener('SiriInterruptionEnded', (event: SiriInterruptionEvent) => {
      console.log('ðŸŸ¢ LEGACY: Siri interruption ended');
      setIsAudioServicesSuspended(false);
    });

    audioSessionService.addListener('AudioInterruptionBegan', (event: AudioInterruptionEvent) => {
      console.log('ðŸ“± LEGACY: General audio interruption began');
      setIsAudioServicesSuspended(true);
    });

    audioSessionService.addListener('AudioInterruptionEnded', (event: AudioInterruptionEvent) => {
      console.log('ðŸŸ¢ LEGACY: General audio interruption ended');
      setIsAudioServicesSuspended(false);
    });

    // Start enhanced audio streaming
    try {
      const streamResult = await audioSessionService.startAudioStreaming();
      if (streamResult.success) {
        console.log('âœ… Enhanced audio streaming started');
      } else {
        console.error('âŒ Failed to start enhanced audio streaming:', streamResult.message);
      }

      // ðŸ”¥ ENHANCED: Start speech recognition for safe word detection
      console.log('ðŸ—£ï¸ Starting enhanced speech recognition for safe word detection...');
      const speechResult = await audioSessionService.startSpeechRecognition();
      if (speechResult.success) {
        console.log('âœ… Enhanced speech recognition started');
      } else {
        console.error('âŒ Failed to start enhanced speech recognition');
      }

    } catch (error) {
      console.error('âŒ Failed to start enhanced audio services:', error);
    }

    // Store listener for cleanup
    audioStreamRef.current = true;

    // ðŸ”¥ EXACT SAME MESSAGE HANDLING AS OLD APPROACH
    ws.current.onmessage = event => {
      console.log('Message from server: Background', event.data);
      const parsedData = JSON.parse(event.data);
      const isThreat = parsedData['threat detected'];
      console.log('Message from server:Background===', parsedData);
      if (isThreat) {
        console.log(`${isThreat} is a threatening word.`);
        threatDetected();
      } else {
        console.log('No threat detected.');
      }
    };

    ws.current.onerror = error => {
      console.error('WebSocket error:', error);
    };

    ws.current.onclose = () => {
      console.log('WebSocket connection closed');
    };

    // ðŸ”¥ EXACT SAME BACKGROUND SERVICE LOGIC AS OLD APPROACH
    if (isSafeWord && !isSafeZone) {
      await BackgroundService.start(backgroundTask, options);
    } else {
      console.log('In BG');
      await BackgroundService.stop();
    }

    const isServiceActive = await BackgroundService.isRunning();
    if (isServiceActive) {
      console.log('Background service is active.');
      // onDisplayNotification('Rove', 'Rove is active');
    } else {
      console.log('Background service is not active.');
    }
  }, [currentModel, isSafeZone, handleMicrophoneInterruption, handleDebugLog, isAudioServicesSuspended]);

  // ðŸ”¥ EXACT SAME MAIN EFFECT AS OLD APPROACH
  useEffect(() => {
    console.log('ðŸ“‹ Main effect triggered with enhanced interruption handling conditions:', {
      isSafeWord,
      isSafeZone,
      currentModel,
      safeWord,
      audioNameString,
      isAudioServicesSuspended
    });

    if (isSafeWord && !isSafeZone) {
      console.log('âœ… Conditions met - starting enhanced streaming...');
      startStreaming();
    } else {
      console.log('âŒ Conditions not met - skipping enhanced streaming');
      console.log('  - isSafeWord:', isSafeWord);
      console.log('  - isSafeZone:', isSafeZone);
    }

    // Cleanup function for enhanced audio session service
    return () => {
      console.log('ðŸ§¹ Cleaning up enhanced audio session listeners');
      audioSessionService.removeAllListeners();
    };
  }, [startStreaming, isSafeWord, isSafeZone]);

  // Voice detection UI
  const showVoiceDetectionUI = (duration: number = 3000) => {
    setShowVoiceUI(true);
    setVoiceUIState('detected');
    setTimeout(() => {
      setShowVoiceUI(false);
      setVoiceUIState('listening');
    }, duration);
  };

  const siriBlockedModalConfig = {
    title: "Siri access blocked",
    isImageIcon: true,
    content: (
      <View>
        <CustomText.RegularText customStyle={styles.helpModalText}>
          Siri voice activation won't work while Rove is active. This is because Rove needs exclusive access to the microphone for threat detection.
        </CustomText.RegularText>
        <CustomText.RegularText customStyle={styles.helpModalText}>
          To use Siri, press and hold the side button on your device.
        </CustomText.RegularText>
      </View>
    )
  };

  // UI helper functions
  const getIconSize = (iconName: string) => {
    switch (iconName) {
      case 'LiveStream': return { width: 30, height: 30 };
      case 'HeadsUp': return { width: 30, height: 30 };
      case 'Premium': return { width: 30, height: 30 };
      case 'Settings': return { width: 24, height: 24 };
      default: return { width: 25, height: 25 };
    }
  };

  const renderTabIcon = ({ color, focused }: any, item: any) => {
    const iconSize = getIconSize(item?.name);
    const showAlert = item?.name === 'HeadsUp' && threatAlertMode && hasThreatsInRadius;

    return (
      <View style={styles.tabIconContainer}>
        <Image
          source={focused ? item?.active : item?.inActive}
          resizeMode="contain"
          style={{
            tintColor: color,
            width: Metrix.HorizontalSize(iconSize.width),
            height: Metrix.VerticalSize(iconSize.height),
          }}
        />
        {showAlert && (
          <View style={styles.alertIndicator}>
            <CustomText.SmallText customStyle={styles.alertText}>
              !
            </CustomText.SmallText>
          </View>
        )}
      </View>
    );
  };


  return (
    <>
      <PaperProvider
        theme={{
          ...MD3LightTheme,
          colors: {
            ...MD3LightTheme.colors,
            surface: Utills.selectedThemeColors().Base,
          }
        }}
      >
        <Tab.Navigator
          initialRouteName='LiveStream'
          screenOptions={{
            headerShown: false,
            tabBarStyle: {
              position: (isVideoMode && isOnLiveStreamTab) ? 'absolute' : 'relative',
              bottom: (isVideoMode && isOnLiveStreamTab) ? 0 : undefined,
              backgroundColor: (isVideoMode && isOnLiveStreamTab) ? 'transparent' : 'rgba(0, 0, 0, 0.9)',
              left: 0,
              right: 0,
              shadowOpacity: 0,
              borderTopWidth: 0,
              elevation: 0,
              height: 90,
              paddingBottom: 30,
              paddingTop: 10,
            },
            tabBarActiveTintColor: '#FFF',
            tabBarInactiveTintColor: '#FFF',
            tabBarLabelStyle: {
              fontSize: 12,
              fontWeight: '600',
              marginTop: 4,
              color: '#FFF'
            },
            tabBarIconStyle: {
              marginBottom: -4,
            },
          }}
        >
          {tabsData.map((item) => (
            <Tab.Screen
              key={item.name}
              name={item.name}
              component={item.component}
              options={{
                tabBarLabel: item.name,
                tabBarIcon: ({ color, focused }) => renderTabIcon({ color, focused }, item),
              }}
            />
          ))}
          <Tab.Screen name="TrustedContacts" component={TrustedContacts} options={{ tabBarButton: () => null }} />
          <Tab.Screen name="SafeWord" component={SafeWord} options={{ tabBarButton: () => null }} />
          <Tab.Screen name="SafeWordTraining" component={SafeWordTraining} options={{ tabBarButton: () => null }} />
          <Tab.Screen name="PasscodeSettings" component={PasscodeSettings} options={{ tabBarButton: () => null }} />
          <Tab.Screen name="HowToUse" component={HowToUse} options={{ tabBarButton: () => null }} />
          <Tab.Screen name="Footages" component={Footages} options={{ tabBarButton: () => null }} />
          <Tab.Screen name="BodyCam" component={BodyCam} options={{ tabBarButton: () => null }} />
          <Tab.Screen name="HeadsUpSettings" component={HeadsUpSettings} options={{ tabBarButton: () => null }} />
          <Tab.Screen name="SiriSetupScreen" component={SiriSetupScreen} options={{ tabBarButton: () => null }} />
        </Tab.Navigator>
      </PaperProvider>

      <CustomModal
        visible={isVisible}
        smallModal
        onClose={() => setIsVisible(false)}>
        <CustomText.MediumText customStyle={{ letterSpacing: 0.9 }}>
          Are you being threatened ?
        </CustomText.MediumText>
        <View style={styles.modalButtonContainer}>
          <PrimaryButton
            title="Yes"
            customStyles={{ borderRadius: 10 }}
            width={'45%'}
            onPress={() => {
              console.log('ðŸš¨ User confirmed threat - starting stream...');
              setIsVisible(false);
              onDisplayNotification('A New Threat Detected', 'Start your live stream now');
              NavigationService.navigate('LiveStream', { triggerFunction: true });
            }}
          />
          <PrimaryButton
            title="No"
            width={'45%'}
            customStyles={{ borderRadius: 10 }}
            onPress={() => {
              console.log('âœ… User denied threat - closing modal');
              setIsVisible(false);
            }}
          />
        </View>
      </CustomModal>

      {showSiriBlockedModal && (
        <View style={styles.helpModalOverlay}>
          <View style={styles.helpModalContainer}>
            {/* Close button */}
            <TouchableOpacity
              style={styles.helpModalCloseButton}
              onPress={() => setShowSiriBlockedModal(false)}
            >
              <Text style={styles.helpModalCloseText}>âœ•</Text>
            </TouchableOpacity>

            {/* Header */}
            <View style={styles.helpModalHeader}>
              <CustomText.MediumText customStyle={styles.helpModalTitle}>
                {siriBlockedModalConfig.title}
              </CustomText.MediumText>
            </View>

            {/* Divider */}
            <View style={styles.helpModalDivider} />

            {/* Content */}
            <View style={styles.helpModalContentContainer}>
              {siriBlockedModalConfig.content}
            </View>

            {/* Got It button */}
            <View style={{ alignItems: 'center', marginTop: 15 }}>
              <PrimaryButton
                title="Got It"
                customStyles={{ borderRadius: 10 }}
                width={'60%'}
                onPress={() => {
                  console.log('User acknowledged Siri blocking');
                  setShowSiriBlockedModal(false);
                }}
              />
            </View>
          </View>
        </View>
      )}

      <VoiceDetectionUI
        visible={showVoiceUI}
        isListening={voiceUIState === 'listening'}
        audioLevel={audioLevel}
        onAnimationComplete={() => {
          console.log('Voice UI animation completed');
          setShowVoiceUI(false);
        }}
      />

      {showFakeLockScreen && (
        <FakeLockScreen
          visible={showFakeLockScreen}
          onUnlock={() => {
            dispatch(HomeActions.setShowFakeLockScreen(false));
          }}
        />
      )}
    </>
  );
};

const styles = StyleSheet.create({
  barStyle: {
    borderTopWidth: 0,
    height: Metrix.VerticalSize(110),
    paddingTop: Metrix.VerticalSize(15),
    paddingBottom: Metrix.VerticalSize(15),
    shadowColor: Utills.selectedThemeColors().PrimaryTextColor,
    shadowOffset: {
      width: Metrix.HorizontalSize(3),
      height: Metrix.VerticalSize(2),
    },
    shadowOpacity: 0.1,
    shadowRadius: 20,
    elevation: Metrix.VerticalSize(20),
  },
  modalButtonContainer: {
    width: '75%',
    flexDirection: 'row',
    justifyContent: 'space-between',
    marginTop: Metrix.VerticalSize(10),
  },
  tabIconContainer: {
    position: 'relative',
    alignItems: 'center',
    justifyContent: 'center',
  },
  alertIndicator: {
    position: 'absolute',
    top: -8,
    right: -8,
    width: Metrix.HorizontalSize(16),
    height: Metrix.VerticalSize(16),
    backgroundColor: '#FF3B30',
    borderRadius: Metrix.HorizontalSize(8),
    alignItems: 'center',
    justifyContent: 'center',
  },
  alertIcon: {
    width: Metrix.HorizontalSize(10),
    height: Metrix.VerticalSize(10),
    tintColor: '#FFFFFF',
  },
  alertText: {
    color: '#FFFFFF',
    fontSize: Metrix.customFontSize(12),
    fontWeight: 'bold',
    textAlign: 'center',
    lineHeight: Metrix.VerticalSize(12),
  },
  helpModalOverlay: {
    position: 'absolute',
    top: 0,
    left: 0,
    right: 0,
    bottom: 0,
    backgroundColor: 'rgba(0, 0, 0, 0.8)',
    zIndex: 1000,
    alignItems: 'center',
    justifyContent: 'center',
    paddingHorizontal: 20,
  },

  helpModalContainer: {
    backgroundColor: '#141414',
    borderRadius: 16,
    padding: 20,
    maxWidth: '100%',
    minWidth: '80%',
    position: 'relative',
  },

  helpModalCloseButton: {
    position: 'absolute',
    top: 0,
    right: 5,
    width: 40,
    height: 40,
    alignItems: 'center',
    justifyContent: 'center',
    zIndex: 1001,
  },

  helpModalCloseText: {
    color: '#8E8E93',
    fontSize: 18,
    fontWeight: 'bold',
  },

  helpModalHeader: {
    flexDirection: 'row',
    alignItems: 'center',
    marginBottom: 5,
  },

  helpModalHeaderImage: {
    width: 38,
    height: 35,
    marginRight: 12,
    marginLeft: -4,
    tintColor: '#FFFFFF',
  },

  helpModalTitle: {
    color: '#FFFFFF',
    fontSize: 20,
    fontWeight: '600',
    flex: 1,
    marginLeft: 0
  },

  helpModalDivider: {
    height: 1,
    backgroundColor: '#48484A',
    marginBottom: 10,
  },

  helpModalContentContainer: {
    paddingRight: 10,
  },

  helpModalText: {
    color: '#FFFFFF',
    fontSize: 18,
    lineHeight: 20,
    marginBottom: 10,
  },

  highlightedText: {
    color: '#57b5fa',
    fontWeight: '600',
  },
});